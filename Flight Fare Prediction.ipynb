{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_path_exist(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"please check the path\")\n",
    "    else: \n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Machine Learning\\flight-price-prediction\\Dataset\\Data_Train.xlsx\n",
      "F:\\Machine Learning\\flight-price-prediction\\Dataset\\Test_set.xlsx\n"
     ]
    }
   ],
   "source": [
    "# project_path = os.getcwd()\n",
    "project_path = \"F:\\\\Machine Learning\\\\flight-price-prediction\\\\\"\n",
    "train_dataset_path = os.path.join(project_path ,\"Dataset\\\\Data_Train.xlsx\")\n",
    "test_dataset_path = os.path.join(project_path ,\"Dataset\\\\Test_set.xlsx\")\n",
    "is_path_exist(train_dataset_path)\n",
    "is_path_exist(test_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Date_of_Journey</th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Route</th>\n",
       "      <th>Dep_Time</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Additional_Info</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>24/03/2019</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → DEL</td>\n",
       "      <td>22:20</td>\n",
       "      <td>01:10 22 Mar</td>\n",
       "      <td>2h 50m</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>3897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air India</td>\n",
       "      <td>1/05/2019</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → IXR → BBI → BLR</td>\n",
       "      <td>05:50</td>\n",
       "      <td>13:15</td>\n",
       "      <td>7h 25m</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>7662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jet Airways</td>\n",
       "      <td>9/06/2019</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>DEL → LKO → BOM → COK</td>\n",
       "      <td>09:25</td>\n",
       "      <td>04:25 10 Jun</td>\n",
       "      <td>19h</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>13882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>12/05/2019</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → NAG → BLR</td>\n",
       "      <td>18:05</td>\n",
       "      <td>23:30</td>\n",
       "      <td>5h 25m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>6218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>01/03/2019</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → NAG → DEL</td>\n",
       "      <td>16:50</td>\n",
       "      <td>21:35</td>\n",
       "      <td>4h 45m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>13302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Airline Date_of_Journey    Source Destination                  Route  \\\n",
       "0       IndiGo      24/03/2019  Banglore   New Delhi              BLR → DEL   \n",
       "1    Air India       1/05/2019   Kolkata    Banglore  CCU → IXR → BBI → BLR   \n",
       "2  Jet Airways       9/06/2019     Delhi      Cochin  DEL → LKO → BOM → COK   \n",
       "3       IndiGo      12/05/2019   Kolkata    Banglore        CCU → NAG → BLR   \n",
       "4       IndiGo      01/03/2019  Banglore   New Delhi        BLR → NAG → DEL   \n",
       "\n",
       "  Dep_Time  Arrival_Time Duration Total_Stops Additional_Info  Price  \n",
       "0    22:20  01:10 22 Mar   2h 50m    non-stop         No info   3897  \n",
       "1    05:50         13:15   7h 25m     2 stops         No info   7662  \n",
       "2    09:25  04:25 10 Jun      19h     2 stops         No info  13882  \n",
       "3    18:05         23:30   5h 25m      1 stop         No info   6218  \n",
       "4    16:50         21:35   4h 45m      1 stop         No info  13302  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_excel(train_dataset_path)\n",
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Price</th>\n",
       "      <th>Journey_day</th>\n",
       "      <th>Journey_month</th>\n",
       "      <th>Arrival_hour</th>\n",
       "      <th>Arrival_min</th>\n",
       "      <th>Departure_hour</th>\n",
       "      <th>Departure_min</th>\n",
       "      <th>Durations_hours</th>\n",
       "      <th>Durations_mins</th>\n",
       "      <th>...</th>\n",
       "      <th>Vistara Premium economy</th>\n",
       "      <th>Chennai</th>\n",
       "      <th>Delhi</th>\n",
       "      <th>Kolkata</th>\n",
       "      <th>Mumbai</th>\n",
       "      <th>Cochin</th>\n",
       "      <th>Delhi</th>\n",
       "      <th>Hyderabad</th>\n",
       "      <th>Kolkata</th>\n",
       "      <th>New Delhi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3897</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7662</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13882</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6218</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13302</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total_Stops  Price  Journey_day  Journey_month  Arrival_hour  Arrival_min  \\\n",
       "0            0   3897           24              3             1           10   \n",
       "1            2   7662            1              5            13           15   \n",
       "2            2  13882            9              6             4           25   \n",
       "3            1   6218           12              5            23           30   \n",
       "4            1  13302            1              3            21           35   \n",
       "\n",
       "   Departure_hour  Departure_min Durations_hours Durations_mins  ...  \\\n",
       "0              22             20               2             50  ...   \n",
       "1               5             50               7             25  ...   \n",
       "2               9             25              19              0  ...   \n",
       "3              18              5               5             25  ...   \n",
       "4              16             50               4             45  ...   \n",
       "\n",
       "   Vistara Premium economy  Chennai  Delhi  Kolkata  Mumbai  Cochin  Delhi  \\\n",
       "0                        0        0      0        0       0       0      0   \n",
       "1                        0        0      0        1       0       0      0   \n",
       "2                        0        0      1        0       0       1      0   \n",
       "3                        0        0      0        1       0       0      0   \n",
       "4                        0        0      0        0       0       0      0   \n",
       "\n",
       "   Hyderabad  Kolkata  New Delhi  \n",
       "0          0        0          1  \n",
       "1          0        0          0  \n",
       "2          0        0          0  \n",
       "3          0        0          0  \n",
       "4          0        0          1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can not handle date/time directly in our ML model we have to find a way which can hold the information in  numeric way, \n",
    "# So we will add 2 or 3 column for these cols date or time.\n",
    "\n",
    "def convert_date_time(dataset, col_name, prefix = None, extract_date = False, extract_time = False, \n",
    "                      extract_year = False, extract_second = False):\n",
    "    if prefix == None:\n",
    "        prefix = col_name\n",
    "    if extract_date:\n",
    "        dataset[prefix+\"_day\"] = pd.to_datetime(dataset[col_name], format = \"%d/%m/%Y\").dt.day\n",
    "        dataset[prefix+\"_month\"] = pd.to_datetime(dataset[col_name], format = \"%d/%m/%Y\").dt.month\n",
    "        if extract_year:\n",
    "            dataset[prefix+\"_year\"] = pd.to_datetime(dataset[col_name], format = \"%d/%m/%Y\").dt.year\n",
    "    \n",
    "    elif extract_time:\n",
    "        dataset[prefix+\"_hour\"] = pd.to_datetime(dataset[col_name]).dt.hour\n",
    "        dataset[prefix+\"_min\"] = pd.to_datetime(dataset[col_name]).dt.minute\n",
    "        if extract_second:\n",
    "            dataset[prefix+\"_sec\"] = pd.to_datetime(dataset[col_name]).dt.second\n",
    "    dataset.drop([col_name], axis = 1, inplace = True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def convert_duration(dataset,col_name):\n",
    "    duration_hour = []\n",
    "    duration_min =[]\n",
    "    for duration in dataset[col_name]:\n",
    "        duration = duration.split(\" \")\n",
    "        if len(duration) > 1:\n",
    "            duration_hour.append(duration[0].split(\"h\")[0])\n",
    "            duration_min.append(duration[1].split(\"m\")[0])\n",
    "        else:\n",
    "            if \"h\" in duration[0]:\n",
    "                duration_hour.append(duration[0].split(\"h\")[0])\n",
    "                duration_min.append(0)\n",
    "            elif \"m\" in duration[0]:\n",
    "                duration_hour.append(0)\n",
    "                duration_min.append(duration[0].split(\"m\")[0])\n",
    "    dataset[\"Durations_hours\"] = duration_hour \n",
    "    dataset[\"Durations_mins\"] = duration_min\n",
    "    dataset.drop([col_name], axis = 1, inplace = True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def add_dummies(dataset,col_name):\n",
    "    col_data = dataset[col_name]\n",
    "    col_dummies = pd.get_dummies(col_data,drop_first= True)\n",
    "    dataset = pd.concat([dataset,col_dummies],axis = 1)\n",
    "    dataset.drop([col_name], axis = 1, inplace = True)\n",
    "    return dataset\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    dataset.dropna(inplace = True)\n",
    "    dataset = convert_date_time(dataset, \"Date_of_Journey\", prefix = \"Journey\", extract_date=True)\n",
    "    dataset = convert_date_time(dataset, \"Arrival_Time\", prefix = \"Arrival\", extract_time=True)\n",
    "    dataset = convert_date_time(dataset, \"Dep_Time\", prefix = \"Departure\", extract_time=True)\n",
    "    dataset = convert_duration(dataset, \"Duration\")\n",
    "    # As we can see Route and addtional_info column seems less important, so we drop them\n",
    "    dataset.drop([\"Route\",\"Additional_Info\"], axis = 1, inplace = True)\n",
    "    dataset = add_dummies(dataset,\"Airline\")\n",
    "    dataset = add_dummies(dataset,\"Source\")\n",
    "    dataset = add_dummies(dataset,\"Destination\")\n",
    "    dataset.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n",
    "    \n",
    "    return dataset\n",
    "train_dataset = pd.read_excel(train_dataset_path)\n",
    "train_dataset = preprocessing(train_dataset)\n",
    "train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Journey_day</th>\n",
       "      <th>Journey_month</th>\n",
       "      <th>Arrival_hour</th>\n",
       "      <th>Arrival_min</th>\n",
       "      <th>Departure_hour</th>\n",
       "      <th>Departure_min</th>\n",
       "      <th>Durations_hours</th>\n",
       "      <th>Durations_mins</th>\n",
       "      <th>Air India</th>\n",
       "      <th>...</th>\n",
       "      <th>Vistara Premium economy</th>\n",
       "      <th>Chennai</th>\n",
       "      <th>Delhi</th>\n",
       "      <th>Kolkata</th>\n",
       "      <th>Mumbai</th>\n",
       "      <th>Cochin</th>\n",
       "      <th>Delhi</th>\n",
       "      <th>Hyderabad</th>\n",
       "      <th>Kolkata</th>\n",
       "      <th>New Delhi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>23</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total_Stops  Journey_day  Journey_month  Arrival_hour  Arrival_min  \\\n",
       "0            1            6              6             4           25   \n",
       "1            1           12              5            10           20   \n",
       "2            1           21              5            19            0   \n",
       "3            1           21              5            21            0   \n",
       "4            0           24              6             2           45   \n",
       "\n",
       "   Departure_hour  Departure_min Durations_hours Durations_mins  Air India  \\\n",
       "0              17             30              10             55          0   \n",
       "1               6             20               4              0          0   \n",
       "2              19             15              23             45          0   \n",
       "3               8              0              13              0          0   \n",
       "4              23             55               2             50          0   \n",
       "\n",
       "   ...  Vistara Premium economy  Chennai  Delhi  Kolkata  Mumbai  Cochin  \\\n",
       "0  ...                        0        0      1        0       0       1   \n",
       "1  ...                        0        0      0        1       0       0   \n",
       "2  ...                        0        0      1        0       0       1   \n",
       "3  ...                        0        0      1        0       0       1   \n",
       "4  ...                        0        0      0        0       0       0   \n",
       "\n",
       "   Delhi  Hyderabad  Kolkata  New Delhi  \n",
       "0      0          0        0          0  \n",
       "1      0          0        0          0  \n",
       "2      0          0        0          0  \n",
       "3      0          0        0          0  \n",
       "4      1          0        0          0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing\n",
    "test_dataset = pd.read_excel(test_dataset_path)\n",
    "test_dataset = preprocessing(test_dataset)\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_dataset.drop([\"Price\"], axis = 1)\n",
    "labels = train_dataset.Price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-487c53771188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's check important features by using extra_tree_regressior\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mselection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExtraTreesRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's check important features by using extra_tree_regressior\n",
    "selection = ExtraTreesRegressor()\n",
    "selection.fit(features_train, labels_train)\n",
    "print(selection.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "feat_importances = pd.Series(selection.feature_importances_, index=features_train.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting model using Random Forest¶\n",
    "\n",
    "1. Split dataset into train and test set in order to prediction w.r.t X_test\n",
    "2. If needed do scaling of data<br/>\n",
    "    a. Scaling is not done in Random forest\n",
    "3. Import model\n",
    "4. Fit the data\n",
    "5. Predict w.r.t X_test\n",
    "\n",
    "6. In regression check RSME Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, \n",
    "                                            labels, test_size = 0.2, random_state = 42)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(features_train,labels_train)\n",
    "labels_predict = model.predict(features_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy :  0.9533015293443073\n",
      "Testing Accuracy :  0.7963709161352978\n",
      "---------------------------------------------------------------------------\n",
      "MAE: 1180.51323202602\n",
      "MSE: 4390661.775528173\n",
      "RMSE: 2095.3906021379817\n",
      "R2 score : 0.7963709161352978\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy : \", model.score(features_train, labels_train))\n",
    "print(\"Testing Accuracy : \", model.score(features_test, labels_test))\n",
    "\n",
    "print(\"-\"*75)\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(labels_test, labels_predict))\n",
    "print('MSE:', metrics.mean_squared_error(labels_test, labels_predict))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(labels_test, labels_predict)))\n",
    "print(\"R2 score :\", metrics.r2_score(labels_test, labels_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "1. Choose following method for hyperparameter tuning<br/>\n",
    "    a. RandomizedSearchCV --> Fast<br>\n",
    "    b. GridSearchCV\n",
    "2. Assign hyperparameters in form of dictionery\n",
    "3. Fit the model\n",
    "4. Check best paramters and best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10, total=   6.9s\n",
      "[CV] n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10, total=   7.0s\n",
      "[CV] n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10 \n",
      "[CV]  n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10, total=   6.9s\n",
      "[CV] n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10 \n",
      "[CV]  n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10, total=   7.6s\n",
      "[CV] n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10 \n",
      "[CV]  n_estimators=900, min_samples_split=5, min_samples_leaf=5, max_features=sqrt, max_depth=10, total=   6.9s\n",
      "[CV] n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15, total=  10.5s\n",
      "[CV] n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15, total=  10.5s\n",
      "[CV] n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15, total=  10.5s\n",
      "[CV] n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15, total=  10.7s\n",
      "[CV] n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=1100, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=15, total=  11.2s\n",
      "[CV] n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15, total=   6.7s\n",
      "[CV] n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15, total=   6.1s\n",
      "[CV] n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15, total=   6.0s\n",
      "[CV] n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15, total=   6.0s\n",
      "[CV] n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=100, min_samples_leaf=5, max_features=auto, max_depth=15, total=   6.0s\n",
      "[CV] n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15, total=  11.0s\n",
      "[CV] n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15, total=  10.9s\n",
      "[CV] n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15, total=  10.9s\n",
      "[CV] n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15, total=  11.5s\n",
      "[CV] n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15 \n",
      "[CV]  n_estimators=400, min_samples_split=5, min_samples_leaf=5, max_features=auto, max_depth=15, total=  10.9s\n",
      "[CV] n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20, total=  16.9s\n",
      "[CV] n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20, total=  16.9s\n",
      "[CV] n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20, total=  17.6s\n",
      "[CV] n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20, total=  16.7s\n",
      "[CV] n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=5, min_samples_leaf=10, max_features=auto, max_depth=20, total=  16.8s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25, total=  17.3s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25, total=  18.6s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25, total=  16.7s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25, total=  16.8s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features=sqrt, max_depth=25, total=  16.7s\n",
      "[CV] n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5, total=   6.2s\n",
      "[CV] n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5, total=   5.9s\n",
      "[CV] n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5, total=   5.6s\n",
      "[CV] n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5, total=   5.7s\n",
      "[CV] n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1100, min_samples_split=15, min_samples_leaf=10, max_features=sqrt, max_depth=5, total=   5.7s\n",
      "[CV] n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15, total=   2.8s\n",
      "[CV] n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15, total=   2.8s\n",
      "[CV] n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15, total=   2.8s\n",
      "[CV] n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15, total=   2.8s\n",
      "[CV] n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15 \n",
      "[CV]  n_estimators=300, min_samples_split=15, min_samples_leaf=1, max_features=sqrt, max_depth=15, total=   2.8s\n",
      "[CV] n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   3.8s\n",
      "[CV] n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   3.6s\n",
      "[CV] n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   3.6s\n",
      "[CV] n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   3.6s\n",
      "[CV] n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=700, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   3.6s\n",
      "[CV] n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20, total=  21.1s\n",
      "[CV] n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20, total=  20.2s\n",
      "[CV] n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20, total=  20.1s\n",
      "[CV] n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20, total=  20.6s\n",
      "[CV] n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=700, min_samples_split=15, min_samples_leaf=1, max_features=auto, max_depth=20, total=  20.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=1,\n",
       "                   param_distributions={'max_depth': [5, 10, 15, 20, 25, 30],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 5, 10],\n",
       "                                        'min_samples_split': [2, 5, 10, 15,\n",
       "                                                              100],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000, 1100,\n",
       "                                                         1200]},\n",
       "                   random_state=42, scoring='neg_mean_squared_error',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomized Search CV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "model = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, \n",
    "                               scoring='neg_mean_squared_error', n_iter = 10, cv = 5, \n",
    "                               verbose=2, random_state=42, n_jobs = 1)\n",
    "\n",
    "rf_random.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 700,\n",
       " 'min_samples_split': 15,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 20}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1165.1785809562468\n",
      "MSE: 4050581.7853487646\n",
      "RMSE: 2012.6057202911763\n",
      "R2 score : 0.8121430662988388\n"
     ]
    }
   ],
   "source": [
    "labels_predict = rf_random.predict(features_test)\n",
    "print('MAE:', metrics.mean_absolute_error(labels_test, labels_predict))\n",
    "print('MSE:', metrics.mean_squared_error(labels_test, labels_predict))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(labels_test, labels_predict)))\n",
    "print(\"R2 score :\", metrics.r2_score(labels_test, labels_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open('flight_rf.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(rf_random, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
